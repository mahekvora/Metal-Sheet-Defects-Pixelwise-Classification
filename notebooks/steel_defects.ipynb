{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjZfovwmgVsf"
      },
      "source": [
        "# **Importing modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmDH_BDUN0Sa",
        "outputId": "21656f4d-5213-4f34-e9d9-ecc2b546d196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "36G-bq2Uw0qx",
        "outputId": "1a5008f6-12be-4c1c-fb59-979608dbd1e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#confirming gpu connection\n",
        "import tensorflow as tf\n",
        "device_name=tf.test.gpu_device_name()\n",
        "print(device_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H90xAhvbQazR",
        "outputId": "c5296109-4ed9-47e6-e901-4418fe5d5dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.3.1\n",
            "  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
            "\u001b[K     |████████████████████████████████| 377 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.3.1) (1.5.2)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.7.0 requires keras<2.8,>=2.7.0rc0, but you have keras 2.3.1 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.3.1 keras-applications-1.0.8\n",
            "Collecting tensorflow==2.1.0\n",
            "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8 MB 25 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.42.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.13.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.19.5)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.37.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.1.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=2c42140195cc8dfbe78ac39adfb48b997e85a6274a51794e00269754803dcbfc\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n",
            "Requirement already satisfied: keras_applications==1.0.8 in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications==1.0.8) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications==1.0.8) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications==1.0.8) (1.5.2)\n",
            "Collecting image-classifiers==1.0.0\n",
            "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from image-classifiers==1.0.0) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (1.5.2)\n",
            "Installing collected packages: image-classifiers\n",
            "Successfully installed image-classifiers-1.0.0\n",
            "Collecting segmentation-models\n",
            "  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from segmentation-models) (1.0.8)\n",
            "Collecting efficientnet==1.0.0\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: image-classifiers==1.0.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.18.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.5.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.1.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.2.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.15.0)\n",
            "Installing collected packages: efficientnet, segmentation-models\n",
            "Successfully installed efficientnet-1.0.0 segmentation-models-1.0.1\n",
            "Requirement already satisfied: efficientnet==1.0.0 in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0) (0.18.3)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (1.5.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0) (3.2.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras==2.3.1\n",
        "!pip install tensorflow==2.1.0\n",
        "!pip install keras_applications==1.0.8\n",
        "!pip install image-classifiers==1.0.0\n",
        "!pip install segmentation-models\n",
        "!pip install efficientnet==1.0.0\n",
        "!pip install segmentation_models\n",
        "!pip install tta_wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiiA5UjEOMrt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mlp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.options.display.max_columns = None\n",
        "%matplotlib inline\n",
        "mlp.rcParams[\"figure.figsize\"] = (25, 5)\n",
        "plt.style.use('seaborn')\n",
        "import sys \n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "import tensorflow\n",
        "from glob import glob\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "seed = 123\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.python.keras import backend as K \n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import regularizers, optimizers, Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Input\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling2D, Conv2DTranspose, UpSampling2D, Dense, GlobalAveragePooling2D\n",
        "from keras.optimizers import *\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from segmentation_models import Unet\n",
        "from segmentation_models import get_preprocessing\n",
        "from tta_wrapper import tta_segmentation\n",
        "from tensorflow.keras.utils import Sequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuknbQXawdTd"
      },
      "source": [
        "# **Data Pre Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCr3uzJNG_Ps"
      },
      "outputs": [],
      "source": [
        "#to extract from zip files use this\n",
        "\n",
        "\n",
        "#DRIVE_ZIP_DIR = \"/content/drive/My Drive/Datasets/caliche\"\n",
        "# source_file_path = DRIVE_ZIP_DIR+ '/severstal-steel-defect-detection.zip'\n",
        "# print(\"Extracting contents of zip file.\")\n",
        "# zip_ref = zipfile.ZipFile(source_file_path, 'r')\n",
        "# zip_ref.extractall(DRIVE_ZIP_DIR)\n",
        "# zip_ref.close()\n",
        "\n",
        "# print(\"Done..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demqEtvlGFQG"
      },
      "outputs": [],
      "source": [
        "PATH_TO_DATA=\"/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection\"\n",
        "PATH_TO_PROJECT='/content/drive/My Drive/metal-sheet-defect'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hN1GCgAHFga"
      },
      "outputs": [],
      "source": [
        "file_path = PATH_TO_DATA+'/train.csv'\n",
        "raw_data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcVph-DtyGGo"
      },
      "source": [
        "# **Data Cleaning**\n",
        "\n",
        "Handle missing values, outliers, rare values and drop the unnecessary features that do not carry useful information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4d-J7lByQP9"
      },
      "outputs": [],
      "source": [
        "print(\"The number of records are : \", raw_data.shape[0])\n",
        "print(\"The number of features are : \", raw_data.shape[1])\n",
        "print(\"The list of features is : \", raw_data.columns)\n",
        "raw_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKEl9riERv4w"
      },
      "outputs": [],
      "source": [
        "def RLE_To_ImageMask(mask_rle, shape=(1600, 256)):\n",
        "    '''\n",
        "    mask_rle: run-length as string formated (start length)\n",
        "    shape: (width, height) of array to return \n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "\n",
        "    '''\n",
        "    # Splitting the run-length encoding\n",
        "    s = mask_rle.split()\n",
        "    # Creating a np array for start pixel and its length\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    # Reducing the start by 1\n",
        "    starts -= 1\n",
        "    # Creating a np array for end pixel\n",
        "    ends = starts + lengths\n",
        "    # Creating a img mask\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    # Entering 1 at the place for the defect pixels\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "\n",
        "    return img.reshape(shape).T     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hald0fvwy2IG"
      },
      "outputs": [],
      "source": [
        "def Add_Undefected_Images(data): \n",
        "    \"\"\" \n",
        "        Takes the dataframe as input and return the updated dataframe which contain both defected and undefected images information \n",
        "    \"\"\" \n",
        "    # Count the image file names for the given directory\n",
        "    count = 0\n",
        "    folder = \"/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/train_images\"\n",
        "    filename_list = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename not in data[\"ImageId\"].values:\n",
        "            filename_list.append(filename)\n",
        "        count += 1\n",
        "    # Print the total number of images present in the directory\n",
        "    print(\"No. of Training images provided : \", count)\n",
        "    print(\"No. of Undefected images : \", len(filename_list))\n",
        "    \n",
        "    # Creating the dictionary that contain the undefected images details\n",
        "    dictionary = {}\n",
        "    dictionary.update({\"ImageId\": filename_list, \"ClassId\": 0, \"EncodedPixels\": 0, \"mask_pixel_sum\": 0})\n",
        "    data_undefected = pd.DataFrame(dictionary)\n",
        "\n",
        "    # Concatinate the the defected and undefected images in the single dataframe called data.\n",
        "    data = pd.concat([data, data_undefected])\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMNiInCooVWE"
      },
      "outputs": [],
      "source": [
        "# Calculating the sum of defected pixels for the defected image\n",
        "raw_data[\"mask_pixel_sum\"] = raw_data.apply(lambda x: RLE_To_ImageMask(x[\"EncodedPixels\"]).sum(), axis = 1)\n",
        "# Adding details for the undefected images\n",
        "raw_data = Add_Undefected_Images(raw_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntWxmYXm0stM"
      },
      "outputs": [],
      "source": [
        "raw_data[\"binary_class\"] = raw_data[\"ClassId\"].apply(lambda x: 0 if x == 0 else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhFqDCNc0w2U"
      },
      "outputs": [],
      "source": [
        "# Checking the features and no. of records in the dataset after including the details for missing images\n",
        "\n",
        "print(\"The number of records are : \", raw_data.shape[0])\n",
        "print(\"The number of features are : \", raw_data.shape[1])\n",
        "print(\"The list of features is : \", raw_data.columns)\n",
        "\n",
        "raw_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9shit7Pi06Pw"
      },
      "outputs": [],
      "source": [
        "print(\"No. of images with multiple class of defects is : \", raw_data.shape[0] - 12568)\n",
        "print(\"The basic info about the raw data is : \\n\")\n",
        "raw_data.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXuQPakn1NWi"
      },
      "outputs": [],
      "source": [
        "# Checking the missing values in the dataset\n",
        "print(\"No. of missing values in the dataset : \\n\", raw_data.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B5pscU311U2"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU7bqZm414Wf"
      },
      "outputs": [],
      "source": [
        "# Plotting a countplot to visualize the distribution of reviews based on Rating Score\n",
        "df_binary_clf = raw_data[[\"ImageId\", \"binary_class\"]].drop_duplicates()\n",
        "sns.set(style=\"darkgrid\")\n",
        "fig, axes = plt.subplots(1, 1, figsize=(25, 5))\n",
        "sns.countplot(x=\"binary_class\", data=df_binary_clf, ax=axes)\n",
        "plt.title(\"DEFECT vs NO DEFECT\", fontsize='xx-large')\n",
        "plt.xlabel(\"CLASSES\")\n",
        "plt.ylabel(\"BINARY CLASS COUNT\")\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puXoIpQq3SxW"
      },
      "source": [
        "# **Analysing the training data-Types of defects & distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoWrVUNG3ay0"
      },
      "outputs": [],
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "fig, axes = plt.subplots(1, 1, figsize=(25, 5))\n",
        "\n",
        "sns.countplot(x=\"ClassId\", data=raw_data, ax=axes)\n",
        "\n",
        "plt.title(\"DISTRIBUTION OF DEFECT CLASS IN THE TRAINING DATA\", fontsize='xx-large')\n",
        "plt.xlabel(\"CLASSES\")\n",
        "plt.ylabel(\"DEFFECT CLASS COUNT\")\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ7FGxgY3tjW"
      },
      "source": [
        "# **Scatter Plat for Mask Pixel Sum**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cenOXZC32I4"
      },
      "source": [
        "Defect1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c1kCvfQ3ztX"
      },
      "outputs": [],
      "source": [
        "a = [i for i in raw_data[raw_data[\"ClassId\"] == 1][\"mask_pixel_sum\"].values]\n",
        "a = sorted(a)\n",
        "\n",
        "sns.set(style=\"darkgrid\")\n",
        "fig, axes = plt.subplots(1, 1, figsize=(25, 5))\n",
        "\n",
        "plt.title(\"Defect Class 1 Mask Sum Threshold Graph\")\n",
        "plt.scatter(a, range(len(a)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h5ezhid4Ctl"
      },
      "source": [
        "Defect 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrqUSCbG4Bt0"
      },
      "outputs": [],
      "source": [
        "a = [i for i in raw_data[raw_data[\"ClassId\"] == 2][\"mask_pixel_sum\"].values]\n",
        "a = sorted(a)\n",
        "\n",
        "sns.set(style=\"darkgrid\")\n",
        "fig, axes = plt.subplots(1, 1, figsize=(25, 5))\n",
        "\n",
        "plt.title(\"Defect Class 1 Mask Sum Threshold Graph\")\n",
        "plt.scatter(a, range(len(a)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EuOTy154LrZ"
      },
      "source": [
        "Defect3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzXLJYGN4GhU"
      },
      "outputs": [],
      "source": [
        "a = [i for i in raw_data[raw_data[\"ClassId\"] == 3][\"mask_pixel_sum\"].values]\n",
        "a = sorted(a)\n",
        "\n",
        "sns.set(style=\"darkgrid\")\n",
        "fig, axes = plt.subplots(1, 1, figsize=(25, 5))\n",
        "\n",
        "plt.title(\"Defect Class 1 Mask Sum Threshold Graph\")\n",
        "plt.scatter(a, range(len(a)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp5r1c_g4QN_"
      },
      "source": [
        "Defect4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdZLvxAy4O1A"
      },
      "outputs": [],
      "source": [
        "a = [i for i in raw_data[raw_data[\"ClassId\"] == 4][\"mask_pixel_sum\"].values]\n",
        "a = sorted(a)\n",
        "\n",
        "sns.set(style=\"darkgrid\")\n",
        "fig, axes = plt.subplots(1, 1, figsize=(25, 5))\n",
        "\n",
        "plt.title(\"Defect Class 1 Mask Sum Threshold Graph\")\n",
        "plt.scatter(a, range(len(a)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10EShsE14Sqa"
      },
      "outputs": [],
      "source": [
        "def Show_Images(dataset, defect_type, start, end):\n",
        "    \"\"\"\n",
        "        Showing the images of different defect type.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Defect Class {defect_type}\")\n",
        "    image_1 = dataset[dataset[\"ClassId\"] == defect_type][start:end][\"ImageId\"].values\n",
        "    for i in image_1:\n",
        "        print(\"Image id : \", i)\n",
        "        sns.set(style=\"darkgrid\")\n",
        "        plt.figure(figsize=(20, 4))\n",
        "        # Loading the images one by one from the directory.\n",
        "        image_read = cv2.imread(\"/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/train_images/\" + i)\n",
        "        image_read = cv2.cvtColor(image_read, cv2.COLOR_BGR2RGB)\n",
        "        # Show the graph\n",
        "        plt.imshow(image_read) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMUPLHZg5jch"
      },
      "source": [
        "class 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ruckyhl5brs"
      },
      "outputs": [],
      "source": [
        "Show_Images(raw_data, 0, 20, 24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wAxieDA6Mdg"
      },
      "source": [
        "class 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0po2EVA5nk3"
      },
      "outputs": [],
      "source": [
        "Show_Images(raw_data, 1, 20, 24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UTn2RaM6SJR"
      },
      "source": [
        "class 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijWyUqKK6Oge"
      },
      "outputs": [],
      "source": [
        "Show_Images(raw_data, 2, 20, 24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phGFky1v6aBX"
      },
      "source": [
        "class 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk6tdJuq6T53"
      },
      "outputs": [],
      "source": [
        "Show_Images(raw_data, 3, 20, 24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRdctFWd6nmb"
      },
      "source": [
        "class 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GaxE7pb6eLi"
      },
      "outputs": [],
      "source": [
        "Show_Images(raw_data, 4, 7, 11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ojYXCmK63__"
      },
      "source": [
        "# **View Masking**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuSCTS3c6vBK"
      },
      "outputs": [],
      "source": [
        "#for def1\n",
        "Encoded = raw_data[raw_data[\"ClassId\"] == 1][\"EncodedPixels\"].values[35]\n",
        "print(\"Masked Image Defect Class 1\")\n",
        "\n",
        "Masked_image = RLE_To_ImageMask(Encoded)\n",
        "plt.figure(figsize=(20, 4))\n",
        "plt.title(\"Sample Image Mask for Defect Class 1\", fontsize='x-large')\n",
        "plt.imshow(Masked_image, cmap=\"viridis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HMdvo5b7Nlb"
      },
      "outputs": [],
      "source": [
        "#for def2\n",
        "Encoded = raw_data[raw_data[\"ClassId\"] == 2][\"EncodedPixels\"].values[35]\n",
        "print(\"Masked Image Defect Class 1\")\n",
        "\n",
        "Masked_image = RLE_To_ImageMask(Encoded)\n",
        "plt.figure(figsize=(20, 4))\n",
        "plt.title(\"Sample Image Mask for Defect Class 1\", fontsize='x-large')\n",
        "plt.imshow(Masked_image, cmap=\"viridis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7HYVMMs7Zk9"
      },
      "outputs": [],
      "source": [
        "#for def3\n",
        "Encoded = raw_data[raw_data[\"ClassId\"] == 3][\"EncodedPixels\"].values[35]\n",
        "print(\"Masked Image Defect Class 1\")\n",
        "\n",
        "Masked_image = RLE_To_ImageMask(Encoded)\n",
        "plt.figure(figsize=(20, 4))\n",
        "plt.title(\"Sample Image Mask for Defect Class 1\", fontsize='x-large')\n",
        "plt.imshow(Masked_image, cmap=\"viridis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBgsrUFM7dIU"
      },
      "outputs": [],
      "source": [
        "#for def4\n",
        "Encoded = raw_data[raw_data[\"ClassId\"] == 4][\"EncodedPixels\"].values[35]\n",
        "print(\"Masked Image Defect Class 1\")\n",
        "\n",
        "Masked_image = RLE_To_ImageMask(Encoded)\n",
        "plt.figure(figsize=(20, 4))\n",
        "plt.title(\"Sample Image Mask for Defect Class 1\", fontsize='x-large')\n",
        "plt.imshow(Masked_image, cmap=\"viridis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPcy523s7nUa"
      },
      "source": [
        "# **Feature engineering to prepare the data for algorithms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6WeQxzS7faJ"
      },
      "outputs": [],
      "source": [
        "# Creating a new feature by combining the image name and class\n",
        "raw_data[\"ImageId_ClassId\"] = raw_data[\"ImageId\"] + \"_\" + raw_data[\"ClassId\"].astype(str) \n",
        "# Sorting the df based on image name and class\n",
        "raw_data = raw_data.sort_values(by=[\"ImageId_ClassId\"], axis=0, ignore_index=True)\n",
        "raw_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wmFZOrN8EqO"
      },
      "outputs": [],
      "source": [
        "# Grouping the ImageId together to extract IDs with multiple defect classes\n",
        "img_class_data = raw_data.groupby([\"ImageId\"])[\"ClassId\"].agg([\"unique\"]).reset_index()\n",
        "\n",
        "# Renaming the feature to \"Class\"\n",
        "img_class_data.rename(columns = {\"unique\": \"Class\"}, inplace = True)\n",
        "\n",
        "# Adding feature for each class\n",
        "img_class_data[\"Class_0\"] = 0\n",
        "img_class_data[\"Class_1\"] = 0\n",
        "img_class_data[\"Class_2\"] = 0\n",
        "img_class_data[\"Class_3\"] = 0\n",
        "img_class_data[\"Class_4\"] = 0\n",
        "\n",
        "# Boolean feature indicating if defect exists\n",
        "img_class_data[\"Any_Class\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxD2Pb2-9X26"
      },
      "outputs": [],
      "source": [
        "# Entering 1 in the respective Class feature and binary \"Any_Class\" feature\n",
        "\n",
        "for idx, row in img_class_data.iterrows():\n",
        "    if 0 in row.Class:\n",
        "        img_class_data.at[idx, 'Class_0'] = 1\n",
        "    if 1 in row.Class:\n",
        "        img_class_data.at[idx, 'Class_1'] = 1\n",
        "        img_class_data.at[idx, 'Any_Class'] = 1\n",
        "    if 2 in row.Class:\n",
        "        img_class_data.at[idx, 'Class_2'] = 1\n",
        "        img_class_data.at[idx, 'Any_Class'] = 1\n",
        "    if 3 in row.Class:\n",
        "        img_class_data.at[idx, 'Class_3'] = 1\n",
        "        img_class_data.at[idx, 'Any_Class'] = 1\n",
        "    if 4 in row.Class:\n",
        "        img_class_data.at[idx, 'Class_4'] = 1\n",
        "        img_class_data.at[idx, 'Any_Class'] = 1\n",
        "\n",
        "img_class_data[\"Sum_of_Defects\"] = img_class_data[\"Class_1\"] + img_class_data[\"Class_2\"] + img_class_data[\"Class_3\"] + img_class_data[\"Class_4\"]\n",
        "img_class_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpmXr8oT9tzD"
      },
      "outputs": [],
      "source": [
        "# basic informations regarding classes\n",
        "\n",
        "print(\"no. of unique Images is : \", len(img_class_data))\n",
        "\n",
        "print(\"no. of Images with defects is : \", len(img_class_data[img_class_data[\"Any_Class\"] == 1]))\n",
        "print(\"no. of Images without defects is : \", len(img_class_data[img_class_data[\"Any_Class\"] == 0]))\n",
        "\n",
        "print(\"no. of Images with Class Defect 1 is : \", len(img_class_data[(img_class_data[\"Class_1\"] == 1) & (img_class_data[\"Sum_of_Defects\"] == 1)]))\n",
        "print(\"no. of Images with Class Defect 2 is : \", len(img_class_data[(img_class_data[\"Class_2\"] == 1) & (img_class_data[\"Sum_of_Defects\"] == 1)]))\n",
        "print(\"no. of Images with Class Defect 3 is : \", len(img_class_data[(img_class_data[\"Class_3\"] == 1) & (img_class_data[\"Sum_of_Defects\"] == 1)]))\n",
        "print(\"no. of Images with Class Defect 4 is : \", len(img_class_data[(img_class_data[\"Class_4\"] == 1) & (img_class_data[\"Sum_of_Defects\"] == 1)]))\n",
        "\n",
        "print(\"no. of Images with 1 Class Defects is : \", len(img_class_data[img_class_data[\"Sum_of_Defects\"] == 1]))\n",
        "print(\"no. of Images with 2 Class Defects is : \", len(img_class_data[img_class_data[\"Sum_of_Defects\"] == 2]))\n",
        "print(\"no. of Images with 3 Class Defects is : \", len(img_class_data[img_class_data[\"Sum_of_Defects\"] == 3]))\n",
        "print(\"no. of Images with 4 Class Defects is : \", len(img_class_data[img_class_data[\"Sum_of_Defects\"] == 4]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7PrHKD4-4_6"
      },
      "source": [
        "# **Training the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Binary Classification**"
      ],
      "metadata": {
        "id": "6VqRVPy9vT_W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfv7bvMczvHX"
      },
      "source": [
        "**Using a smaller model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fEu3oRxLT2a"
      },
      "outputs": [],
      "source": [
        "columns = [\"ImageId\", \"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\", \"Any_Class\", \"Sum_of_Defects\"]\n",
        "y_columns = [\"Any_Class\"]\n",
        "df = img_class_data[columns]\n",
        "#id in the form of npy\n",
        "df['ImageId']=df['ImageId'].map(lambda element: element[:-4]+'.npy')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNNNb9Ljzque"
      },
      "outputs": [],
      "source": [
        "df_small=df.iloc[:5000]\n",
        "df_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D6plBWN0m3d"
      },
      "outputs": [],
      "source": [
        "# Plotting a countplot to visualize the distribution of reviews based on Rating Score \n",
        "#to ensure even distributtion of defected and non-defected images in the smaller dataset\n",
        "sns.set(style=\"darkgrid\")\n",
        "fig, axes = plt.subplots(1, 1, figsize=(25, 5))\n",
        "sns.countplot(x=\"Any_Class\", data=df_small, ax=axes)\n",
        "plt.title(\"DEFECT vs NO DEFECT\", fontsize='xx-large')\n",
        "plt.xlabel(\"CLASSES\")\n",
        "plt.ylabel(\"BINARY CLASS COUNT\")\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ4Fofjz0nA4"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(df_small, stratify=df_small[[\"Any_Class\"]], test_size=0.20, random_state=42)\n",
        "print(\"Train Data Shape :\", train_df.shape, \"Val Data Shape :\", val_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2P5-Stx9h9g"
      },
      "outputs": [],
      "source": [
        "#for the whole dataset use this \n",
        "\n",
        "# train_df, val_df = train_test_split(df, stratify=df[[\"Any_Class\"]], test_size=0.20, random_state=42)\n",
        "# print(\"Train Data Shape :\", train_df.shape, \"Val Data Shape :\", val_df.shape)\n",
        "# #NOTATIONS\n",
        "# partition={'train':train_df['ImageId'].tolist(),'validation':val_df['ImageId'].tolist()}\n",
        "# labels=dict(zip(df.ImageId,df.Any_Class))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3lwGmvO0nGi"
      },
      "outputs": [],
      "source": [
        "partition_small={'train':train_df['ImageId'].tolist(),'validation':val_df['ImageId'].tolist()}\n",
        "labels_small=dict(zip(df_small.ImageId,df_small.Any_Class))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Designing and fitting the model using smaller dataset**"
      ],
      "metadata": {
        "id": "sgS9DgNgwK7P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmUrlbal0uD3"
      },
      "outputs": [],
      "source": [
        "#Data generator\n",
        "class DataGenerator(tensorflow.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=16, dim=(256,1600), n_channels=3,\n",
        "                 n_classes=2, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            size=len(ID)\n",
        "            X[i,] = np.load('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/train_images_np/' + ID)\n",
        "\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID]\n",
        "\n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuGHUirR0uHo"
      },
      "outputs": [],
      "source": [
        "#MODEL 4\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "# Parameters\n",
        "params = {'dim': (256,1600),\n",
        "          'batch_size': 16,\n",
        "          'n_classes': 2,\n",
        "          'n_channels': 3,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition_small['train'], labels_small, **params)\n",
        "validation_generator = DataGenerator(partition_small['validation'], labels_small, **params)\n",
        "\n",
        "\n",
        "# Design model\n",
        "model1 = tensorflow.keras.Sequential()\n",
        "model1.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 1600, 3)))\n",
        "model1.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model1.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "model1.add(keras.layers.Flatten())\n",
        "model1.add(keras.layers.Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model1.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\t# compile model\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train model on dataset\n",
        "history=model1.fit(training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    use_multiprocessing=False,\n",
        "                    epochs=10,\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS4HRwxRW3cH"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tmatplotlib.pyplot.subplot(211)\n",
        "\tmatplotlib.pyplot.title('Cross Entropy Loss')\n",
        "\tmatplotlib.pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tmatplotlib.pyplot.subplot(212)\n",
        "\tmatplotlib.pyplot.title('Classification Accuracy')\n",
        "\tmatplotlib.pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\n",
        "\tmatplotlib.pyplot.savefig('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/' + 'adam_4_10epochs.png')\n",
        "\tmatplotlib.pyplot.close()\n",
        " \n",
        "summarize_diagnostics(history)\n",
        "model1.save('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/model_4_small_data.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T1ChhOJZJvt",
        "outputId": "a68f0000-3a28-4a03-865c-7c3e4206b5c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 250 steps, validate for 62 steps\n",
            "Epoch 11/30\n",
            "250/250 [==============================] - 432s 2s/step - loss: 0.0047 - accuracy: 0.9998 - val_loss: 1.9416 - val_accuracy: 0.7581\n",
            "Epoch 12/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 1.9869 - val_accuracy: 0.7480\n",
            "Epoch 13/30\n",
            "250/250 [==============================] - 228s 913ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 2.0334 - val_accuracy: 0.7470\n",
            "Epoch 14/30\n",
            "250/250 [==============================] - 228s 913ms/step - loss: 0.0061 - accuracy: 0.9995 - val_loss: 1.9857 - val_accuracy: 0.7440\n",
            "Epoch 15/30\n",
            "250/250 [==============================] - 228s 912ms/step - loss: 0.0047 - accuracy: 0.9995 - val_loss: 2.0258 - val_accuracy: 0.7440\n",
            "Epoch 16/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 2.2835 - val_accuracy: 0.7490\n",
            "Epoch 17/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 2.1350 - val_accuracy: 0.7429\n",
            "Epoch 18/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 0.0078 - accuracy: 0.9992 - val_loss: 2.2503 - val_accuracy: 0.7450\n",
            "Epoch 19/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 1.8496 - val_accuracy: 0.7460\n",
            "Epoch 20/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 0.2069 - accuracy: 0.9588 - val_loss: 1.7452 - val_accuracy: 0.7077\n",
            "Epoch 21/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 0.1452 - accuracy: 0.9515 - val_loss: 1.6374 - val_accuracy: 0.6694\n",
            "Epoch 22/30\n",
            "250/250 [==============================] - 228s 910ms/step - loss: 0.2103 - accuracy: 0.9710 - val_loss: 2.3785 - val_accuracy: 0.7288\n",
            "Epoch 23/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 0.0601 - accuracy: 0.9952 - val_loss: 2.0300 - val_accuracy: 0.7490\n",
            "Epoch 24/30\n",
            "250/250 [==============================] - 227s 910ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 2.4262 - val_accuracy: 0.7470\n",
            "Epoch 25/30\n",
            "250/250 [==============================] - 227s 909ms/step - loss: 0.0056 - accuracy: 0.9992 - val_loss: 2.5442 - val_accuracy: 0.7429\n",
            "Epoch 26/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 2.4859 - val_accuracy: 0.7409\n",
            "Epoch 27/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 0.0012 - accuracy: 0.9992 - val_loss: 2.6683 - val_accuracy: 0.7369\n",
            "Epoch 28/30\n",
            "250/250 [==============================] - 227s 910ms/step - loss: 0.0010 - accuracy: 0.9992 - val_loss: 2.7873 - val_accuracy: 0.7389\n",
            "Epoch 29/30\n",
            "250/250 [==============================] - 228s 911ms/step - loss: 9.5197e-04 - accuracy: 0.9992 - val_loss: 2.8793 - val_accuracy: 0.7399\n",
            "Epoch 30/30\n",
            "250/250 [==============================] - 228s 910ms/step - loss: 8.9956e-04 - accuracy: 0.9992 - val_loss: 2.9542 - val_accuracy: 0.7389\n"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model\n",
        "new_model = model1\n",
        "history=new_model.fit(training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    use_multiprocessing=False,\n",
        "                    epochs=30,\n",
        "                  initial_epoch=10)\n",
        "new_model.save(\"updated_model_4_small_data.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVm0uwa8sQkw"
      },
      "outputs": [],
      "source": [
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tmatplotlib.pyplot.subplot(211)\n",
        "\tmatplotlib.pyplot.title('Cross Entropy Loss')\n",
        "\tmatplotlib.pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tmatplotlib.pyplot.subplot(212)\n",
        "\tmatplotlib.pyplot.title('Classification Accuracy')\n",
        "\tmatplotlib.pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\n",
        "\tmatplotlib.pyplot.savefig('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/' + 'updated_adam_4_30epochs.png')\n",
        "\tmatplotlib.pyplot.close()\n",
        " \n",
        "summarize_diagnostics(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCj_L_5ss5Mc"
      },
      "outputs": [],
      "source": [
        "new_model.save('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/updated_model_4_small_data.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PMmGrEx1zaW",
        "outputId": "ccae15e9-d142-4815-a68e-b7210bfcb636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "250/250 [==============================] - 4149s 17s/step - loss: 1966.6694 - accuracy: 0.5203 - val_loss: 0.6925 - val_accuracy: 0.5202\n",
            "Epoch 2/3\n",
            "250/250 [==============================] - 272s 1s/step - loss: 0.6926 - accuracy: 0.5200 - val_loss: 0.6924 - val_accuracy: 0.5202\n",
            "Epoch 3/3\n",
            "250/250 [==============================] - 265s 1s/step - loss: 0.6925 - accuracy: 0.5200 - val_loss: 0.6924 - val_accuracy: 0.5192\n"
          ]
        }
      ],
      "source": [
        "#SMALL MODEL 2\n",
        "#lr 0.001\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot\n",
        "\n",
        "# Parameters\n",
        "params = {'dim': (256,1600),\n",
        "          'batch_size': 16,\n",
        "          'n_classes': 2,\n",
        "          'n_channels': 3,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition_small['train'], labels_small, **params)\n",
        "validation_generator = DataGenerator(partition_small['validation'], labels_small, **params)\n",
        "\n",
        "# Design model\n",
        "model2 = tensorflow.keras.Sequential()\n",
        "model2.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 1600, 3)))\n",
        "model2.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model2.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "model2.add(keras.layers.Flatten())\n",
        "model2.add(keras.layers.Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model2.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\t# compile model\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Train model on dataset\n",
        "history=model2.fit(training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    use_multiprocessing=False,\n",
        "                    epochs=3,\n",
        "                   )\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tmatplotlib.pyplot.subplot(211)\n",
        "\tmatplotlib.pyplot.title('Cross Entropy Loss')\n",
        "\tmatplotlib.pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tmatplotlib.pyplot.subplot(212)\n",
        "\tmatplotlib.pyplot.title('Classification Accuracy')\n",
        "\tmatplotlib.pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tmatplotlib.pyplot.savefig(filename + '_plot.png')\n",
        "\tmatplotlib.pyplot.close()\n",
        " \n",
        "summarize_diagnostics(history)\n",
        "model2.save('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/model_2_small_data.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gv4EIiG2YdA"
      },
      "outputs": [],
      "source": [
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tmatplotlib.pyplot.subplot(211)\n",
        "\tmatplotlib.pyplot.title('Cross Entropy Loss')\n",
        "\tmatplotlib.pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tmatplotlib.pyplot.subplot(212)\n",
        "\tmatplotlib.pyplot.title('Classification Accuracy')\n",
        "\tmatplotlib.pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tmatplotlib.pyplot.savefig(filename + '_plot.png')\n",
        "\tmatplotlib.pyplot.close()\n",
        " \n",
        "summarize_diagnostics(history)\n",
        "model2.save('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/model_2_small_data.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6yBXjzEnz7M"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataGenerator(tensorflow.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=16, dim=(256,1600), n_channels=3,\n",
        "                 n_classes=2, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            size=len(ID)\n",
        "            X[i,] = np.load('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/train_images_np/' + ID)\n",
        "\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID]\n",
        "\n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAmdGBs_0XId",
        "outputId": "aae8859e-a678-423f-e39e-7f9b6a7c6a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "250/250 [==============================] - 4789s 19s/step - loss: 818.4438 - accuracy: 0.5742 - val_loss: 1.5086 - val_accuracy: 0.5262\n",
            "Epoch 2/3\n",
            "250/250 [==============================] - 4648s 19s/step - loss: 1.2233 - accuracy: 0.5210 - val_loss: 1.0974 - val_accuracy: 0.5292\n",
            "Epoch 3/3\n",
            "250/250 [==============================] - 4643s 19s/step - loss: 0.6739 - accuracy: 0.5420 - val_loss: 0.8047 - val_accuracy: 0.5333\n"
          ]
        }
      ],
      "source": [
        "#SMALL MODEL 3\n",
        "#lr 0.0005\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot\n",
        "\n",
        "# Parameters\n",
        "params = {'dim': (256,1600),\n",
        "          'batch_size': 16,\n",
        "          'n_classes': 2,\n",
        "          'n_channels': 3,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition_small['train'], labels_small, **params)\n",
        "validation_generator = DataGenerator(partition_small['validation'], labels_small, **params)\n",
        "\n",
        "# Design model\n",
        "model3 = tensorflow.keras.Sequential()\n",
        "model3.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 1600, 3)))\n",
        "model3.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model3.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "model3.add(keras.layers.Flatten())\n",
        "model3.add(keras.layers.Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model3.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\t# compile model\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "model3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Train model on dataset\n",
        "history=model3.fit(training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    use_multiprocessing=False,\n",
        "                    epochs=3,\n",
        "                   )\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tmatplotlib.pyplot.subplot(211)\n",
        "\tmatplotlib.pyplot.title('Cross Entropy Loss')\n",
        "\tmatplotlib.pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tmatplotlib.pyplot.subplot(212)\n",
        "\tmatplotlib.pyplot.title('Classification Accuracy')\n",
        "\tmatplotlib.pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tmatplotlib.pyplot.savefig(PATH_TO_DATA+'/adam_lr2.png')\n",
        "\tmatplotlib.pyplot.close()\n",
        " \n",
        "summarize_diagnostics(history)\n",
        "model3.save('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/model_3_small_data.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngAGm4sBvuZR"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(tensorflow.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=16, dim=(256,1600), n_channels=3,\n",
        "                 n_classes=2, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            size=len(ID)\n",
        "            X[i,] = np.load('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/train_images_np/' + ID)\n",
        "\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID]\n",
        "\n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsltt5C_uBfM"
      },
      "outputs": [],
      "source": [
        "#########\n",
        "#MODEL 5\n",
        "#VGG 3 layers\n",
        "#Parameters\n",
        "params = {'dim': (256,1600),\n",
        "          'batch_size': 16,\n",
        "          'n_classes': 2,\n",
        "          'n_channels': 3,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition_small['train'], labels_small, **params)\n",
        "validation_generator = DataGenerator(partition_small['validation'], labels_small, **params)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 1600, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history=model.fit(training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    use_multiprocessing=False,\n",
        "                    epochs=5,\n",
        "                   )\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tmatplotlib.pyplot.subplot(211)\n",
        "\tmatplotlib.pyplot.title('Cross Entropy Loss')\n",
        "\tmatplotlib.pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tmatplotlib.pyplot.subplot(212)\n",
        "\tmatplotlib.pyplot.title('Classification Accuracy')\n",
        "\tmatplotlib.pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tmatplotlib.pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tmatplotlib.pyplot.savefig(PATH_TO_DATA+'/adam_lr5.png')\n",
        "\tmatplotlib.pyplot.close()\n",
        "summarize_diagnostics(history)\n",
        "model.save('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/model_5_small_data.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chm1RG9JlnAP"
      },
      "source": [
        "**Convert jpg tp npy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Y7l9HaXLq0Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from numpy import save\n",
        "dir=os.listdir('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/train_images/')\n",
        "dir.sort()\n",
        "for item in dir:\n",
        "  if os.path.isfile('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/train_images/'+item):\n",
        "    img=Image.open('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/train_images/'+item).convert(\"RGB\")\n",
        "    img=np.array(img)\n",
        "    size=len(item)\n",
        "    np.save('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/train_images_np/'+item[:size-4]+'.npy',img)\n",
        "    del(img)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lzRgJyGlwal"
      },
      "source": [
        "**Designing and fitting the model using entire dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_TQBnDNBu0F"
      },
      "outputs": [],
      "source": [
        "#on the whole dataset\n",
        "class DataGenerator(tensorflow.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=16, dim=(256,1600), n_channels=3,\n",
        "                 n_classes=2, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            size=len(ID)\n",
        "            X[i,] = np.load('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/train_images_np/' + ID)\n",
        "\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID]\n",
        "\n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps1KBtbZCwj7",
        "outputId": "f9729379-0ec8-482a-c47f-8ab4d52ab98e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 628 steps, validate for 157 steps\n",
            "628/628 [==============================] - 4347s 7s/step - loss: nan - accuracy: 0.4688 - val_loss: nan - val_accuracy: 0.4697\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6a000d0850>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#MODEL 0\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "# Parameters\n",
        "params = {'dim': (256,1600),\n",
        "          'batch_size': 16,\n",
        "          'n_classes': 2,\n",
        "          'n_channels': 3,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition['train'], labels, **params)\n",
        "validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
        "\n",
        "# Design model\n",
        "model = tensorflow.keras.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 1600, 3)))\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\t# compile model\n",
        "opt = tensorflow.keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train model on dataset\n",
        "model.fit(training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    use_multiprocessing=False\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emf0BIe0hTUC",
        "outputId": "725c4a1f-ae05-49aa-d2a3-cdd7a1e78448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "628/628 [==============================] - 3848s 6s/step - loss: 550.8049 - accuracy: 0.6961 - val_loss: 0.6496 - val_accuracy: 0.7536\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f36550250>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#MODEL 1\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "# Parameters\n",
        "params = {'dim': (256,1600),\n",
        "          'batch_size': 16,\n",
        "          'n_classes': 2,\n",
        "          'n_channels': 3,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition['train'], labels, **params)\n",
        "validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
        "\n",
        "# Design model\n",
        "model1 = tensorflow.keras.Sequential()\n",
        "model1.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 1600, 3)))\n",
        "model1.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model1.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "model1.add(keras.layers.Flatten())\n",
        "model1.add(keras.layers.Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model1.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\t# compile model\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train model on dataset\n",
        "model1.fit(training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    use_multiprocessing=False\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxdKgtwouVlz"
      },
      "outputs": [],
      "source": [
        "#MODEL 2\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "# Parameters\n",
        "params = {'dim': (256,1600),\n",
        "          'batch_size': 16,\n",
        "          'n_classes': 2,\n",
        "          'n_channels': 3,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition['train'], labels, **params)\n",
        "validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
        "\n",
        "# Design model\n",
        "model2 = tensorflow.keras.Sequential()\n",
        "model2.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 1600, 3)))\n",
        "model2.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model2.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "model2.add(keras.layers.Flatten())\n",
        "model2.add(keras.layers.Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model2.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\t# compile model\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        " \n",
        "\n",
        "# Train model on dataset\n",
        "history=model2.fit(training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    use_multiprocessing=False,\n",
        "                    epochs=10,\n",
        "                   )\n",
        "summarize_diagnostics(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd8dkhl988Q4"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/model_0.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf3ZrEUZ0FwF"
      },
      "outputs": [],
      "source": [
        "model1.save('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/model_1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAeSE443uhNW"
      },
      "outputs": [],
      "source": [
        "model2.save('/content/drive/My Drive/Datasets/caliche/severstal-steel-defect-detection/model_2.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "steel_defects.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}